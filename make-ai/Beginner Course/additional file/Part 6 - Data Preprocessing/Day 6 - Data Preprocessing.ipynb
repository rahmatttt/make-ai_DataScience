{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/make ai logo.png\" width=\"180\" height=\"360\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\" style=\"margin-top: 5px\">\n",
    "<h2> In this lesson, we will learn about data preprocessing.\n",
    "\n",
    "Data preprocessing is used to get better data so that machine learning will understand our data.\n",
    "\n",
    "Let's learn. </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#core package for data science\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#import package for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "#hide warnings so you can't find any warning\n",
    "import warnings \n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "print(\"Done !!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For preprocess data, we should divide into numerical and categorical, since that from different data, different ways to tackle that problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As, always for example, we are going to preprocess Titanic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/train.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.fillna({'Age': data['Age'].median()})\n",
    "data = data.fillna({'Embarked': 'S'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'] , axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide your data to numerical and categorical !\n",
    "   - Numerical   : **Age , SibSp , Parch, Fare**\n",
    "   - Categorical : **Pclass, Sex, Embarked**\n",
    "   - We don't have no preprocess **Survived** since that they are Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_num = data[['Age','SibSp','Parch','Fare']]\n",
    "data_cat = data[['Pclass','Sex','Embarked']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_num.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cat.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Data Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, there are two ways to normalization from numerical data. There are **Min-Max Scaler** and **Standard Scaler**.\n",
    "\n",
    "**Min-Max Scaler** takes number of value and then normalize so we get output number between 0 and 1.\n",
    "\n",
    "Example : \n",
    "\n",
    "     data_raw  = [0, 1000, 2000, 3000, 4000]\n",
    "\n",
    "     data_norm = [0, 0.25, 0.5 , 0.75,  1  ]\n",
    "     \n",
    "     \n",
    "     \n",
    "**Standard Scaler** takes number of value and then normalize with normal distribution, refer to **Day 3: Basic Statistics and Math, with Normal Distribution and Z-Scores**, so that we get output number approx. -2 until 2.\n",
    "\n",
    "Example : \n",
    "\n",
    "     data_raw  = [  0  , 1000 , 2000, 3000, 4000]\n",
    "\n",
    "     data_norm = [-1.27, -0.63, 0 , 0.63,  1.27 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/about_standardization_normalization_44_0.png\" width=\"560\" height=\"560\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Import package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am using another package called **sklearn** or Scikit Learn, it is very powerful to create machine learning and some kind of that. More information you can refer to https://scikit-learn.org/stable/modules/preprocessing.html for preprocessing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get Min-Max or Standard, use this package\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Define function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Min-Max\n",
    "scaler1 = MinMaxScaler()\n",
    "\n",
    "# For Standard\n",
    "scaler2 = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Copy your data (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = data_num.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Fitting and Transform your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_m = scaler1.fit_transform(data_1)\n",
    "data_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_s = scaler2.fit_transform(data_1)\n",
    "data_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Convert to DataFrame !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_m = pd.DataFrame(data_m,columns = ['Age','SibSp','Parch','Fare'])\n",
    "data_m.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_s = pd.DataFrame(data_s,columns = ['Age','SibSp','Parch','Fare'])\n",
    "data_s.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, which one do you choose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Data Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, there are two ways to normalization from categorical data. There are **Label Encoder** and **One Hot Encoder**.\n",
    "\n",
    "**Label Encoder** takes categorical value and then convert to label like 0,1,2 etc, sometimes how they get 0,1,2 is random. \n",
    "\n",
    "Example : \n",
    "\n",
    "     data_raw  = [France, Belgium, Germany, France, Germany]\n",
    "\n",
    "     data_norm = [0, 1, 2 ,0, 1]\n",
    "     \n",
    "     \n",
    "     \n",
    "**One Hot Encoder** takes categorical value and convert to number 0 and 1 only, but it create more data.\n",
    "\n",
    "Example : \n",
    "\n",
    "     data_raw  = [France, Belgium, Germany, France, Germany]\n",
    "\n",
    "     data_norm = [[1,0,0],[0,1,0],[0,0,1],[1,0,0],[0,0,1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/0 T5jaa2othYfXZX9W.jpg\" width=\"560\" height=\"560\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Encode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get Min-Max or Standard, use this package\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "print(\"Done !!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Define function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Min-Max\n",
    "encoder1 = LabelEncoder()\n",
    "\n",
    "# For Standard\n",
    "encoder2 = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Copy your data (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2 = data_cat.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Fitting and Transform your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps is almost same, but disadvantages from this is we *can't* encode all of them, we should take one by one column, because..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "encoder1.fit(data_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we should encode one by one to get this done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder1.fit(data_2.iloc[:,2:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to know for which classes they are ordered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder1.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = encoder1.transform(data_2.iloc[:,2:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2.join(a).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't worry, you can still get encode in another way too, with looping !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = data_2.shape[1]\n",
    "col = data_2.columns\n",
    "for i in range(length):\n",
    "    print(i)\n",
    "    a = encoder1.fit_transform(data_2.iloc[:,i:i+1])\n",
    "    a = pd.DataFrame(a, columns=[col[i]+'new'])\n",
    "    data_2 = data_2.join(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some disadvantages too when you are using One Hot Encoder, it is it can't convert any string value.\n",
    "\n",
    "From now, we are using data that we are using with Label Encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new data\n",
    "data_ohe = data_2[[\"Pclassnew\",\"Sexnew\",\"Embarkednew\"]]\n",
    "data_ohe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ohe = encoder2.fit_transform(data_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ohe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we should convert to array to know our data since that just fit and transform is not enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ohe = data_2[[\"Pclassnew\",\"Sexnew\",\"Embarkednew\"]]\n",
    "data_ohe.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ohe = encoder2.fit_transform(data_ohe).toarray()\n",
    "data_ohe = pd.DataFrame(data_ohe, columns=['Pclass_1','Pclass_2','Pclass_3','Sex_female','Sex_male','Embarked_C','Embarked_Q','Embarked_S'])\n",
    "data_ohe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2 = data_2.join(data_ohe)\n",
    "data_2.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, which one do you used? **Label Encoder** or **One Hot Encoder**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join All of your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all of your data, numerical and categorical bring in together !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_num_fix = data_m\n",
    "data_cat_fix = data_2[[\"Pclassnew\",\"Sexnew\",\"Embarkednew\"]]\n",
    "data_fix     = data_num_fix.join(data_cat_fix)\n",
    "data_fix     = data_fix.join(data[[\"Survived\"]])\n",
    "\n",
    "data_fix.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Training and Testing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/1 -8_kogvwmL1H6ooN1A1tsQ.png\" width=\"560\" height=\"560\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For using machine learning, they should have training and testing data since that they need to learn it first, right?\n",
    "\n",
    "So Training for learning data first, after they learning and get good score, which that is a must, they use Testing data to get exam, is it okay for machine learning or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# package that needed for split training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# get split between x(feature) and y(target)\n",
    "X = data_fix.drop(columns='Survived')\n",
    "y = data_fix['Survived']\n",
    "\n",
    "#Split dataset using sklearn\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=45)\n",
    "\n",
    "# Get shape of how many train \n",
    "print (\"Train size : \",X_train.shape)\n",
    "print (\"Test size : \",X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus : How to save our data to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just use **to_csv** to saving your data to csv, so you can import again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate your data !\n",
    "train_data = pd.concat([X_train,y_train],axis=1)\n",
    "test_data = pd.concat([X_test,y_test],axis=1)\n",
    "\n",
    "# Finally, save your data !!\n",
    "train_data.to_csv('train_data.csv',index=False)\n",
    "test_data.to_csv('test_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\" style=\"margin-top: 5px\">\n",
    "<h2> Question </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we are going through diamonds dataset, so we can analyze diamonds by their cut, color, clarity, price, and other attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/diamonds.csv\")\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the description of every columns is like this :\n",
    "\n",
    "**Unnamed:0** : Index counter\n",
    "\n",
    "**carat** : Carat weight of the diamond\n",
    "\n",
    "**cut** :\n",
    "Describe cut quality of the diamond. Quality in increasing order Fair, Good, Very Good, Premium, Ideal\n",
    "\n",
    "**color** :\n",
    "Color of the diamond, with D being the best and J the worst\n",
    "\n",
    "**clarity** :\n",
    "How obvious inclusions are within the diamond:(in order from best to worst, FL = flawless, I3= level 3 inclusions) FL,IF, VVS1, VVS2, VS1, VS2, SI1, SI2, I1, I2, I3\n",
    "\n",
    "**depth** :\n",
    "depth % :The height of a diamond, measured from the culet to the table, divided by its average girdle diameter ( total depth percentage = z / mean(x, y) = 2 * z / (x + y) )\n",
    "\n",
    "**table** :\n",
    "table%: The width of the diamond's table expressed as a percentage of its average diameter\n",
    "\n",
    "**price** :\n",
    "the price of the diamond\n",
    "\n",
    "**x** :\n",
    "length mm\n",
    "\n",
    "**y** :\n",
    "width mm\n",
    "\n",
    "**z** :\n",
    "depth mm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\" style=\"margin-top: 5px\">\n",
    "<h2> Preprocessing this data into something that can divide as Training and Testing Data !!! </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "195.85px",
    "left": "996px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
